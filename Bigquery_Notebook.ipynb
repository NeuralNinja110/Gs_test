{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3040,
          "status": "ok",
          "timestamp": 1744317525233,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "746zYs58kva1",
        "outputId": "7b956d23-e9dc-4c3f-d3a1-9f0336c40014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Requirement already satisfied: copernicusmarine in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: fitz in /usr/local/lib/python3.10/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: frontend in /usr/local/lib/python3.10/dist-packages (0.0.3)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: boto3>=1.26 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (1.37.31)\n",
            "Requirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (8.1.8)\n",
            "Requirement already satisfied: dask>=2022 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (2024.10.0)\n",
            "Requirement already satisfied: h5netcdf<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (2.10.4)\n",
            "Requirement already satisfied: pystac>=1.8.3 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (1.12.2)\n",
            "Requirement already satisfied: semver>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (3.0.4)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (69.5.1)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (4.67.1)\n",
            "Requirement already satisfied: xarray>=2023.4.0 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (2025.1.0)\n",
            "Requirement already satisfied: zarr<3.0.0,>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from copernicusmarine) (2.18.3)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (from fitz) (7.2.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.10/dist-packages (from fitz) (1.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.2.2)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.10/dist-packages (from fitz) (1.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.13.1)\n",
            "Requirement already satisfied: starlette>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (0.46.1)\n",
            "Requirement already satisfied: uvicorn>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from frontend) (0.34.0)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.2.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from frontend) (24.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.1.0)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.31 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.26->copernicusmarine) (1.37.31)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.26->copernicusmarine) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.26->copernicusmarine) (0.11.4)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022->copernicusmarine) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022->copernicusmarine) (2024.10.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022->copernicusmarine) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2022->copernicusmarine) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022->copernicusmarine) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022->copernicusmarine) (8.5.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from h5netcdf<2.0.0,>=1.4.0->copernicusmarine) (3.12.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.1->copernicusmarine) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.1->copernicusmarine) (2.27.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from pystac>=1.8.3->copernicusmarine) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr<3.0.0,>=2.13.3->copernicusmarine) (0.3.3)\n",
            "Requirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr<3.0.0,>=2.13.3->copernicusmarine) (0.13.1)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr<3.0.0,>=2.13.3->copernicusmarine) (0.19)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.2.1)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (6.5.2)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.4.2)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.0.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.20.1)\n",
            "Requirement already satisfied: traits>=6.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (7.0.2)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.16.1)\n",
            "Requirement already satisfied: acres in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: etelemetry>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.28)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.10/dist-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2022->copernicusmarine) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2022->copernicusmarine) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->pystac>=1.8.3->copernicusmarine) (1.17.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium webdriver-manager copernicusmarine fitz frontend pdf2image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 7015,
          "status": "ok",
          "timestamp": 1744317532243,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "Yx6Eo02_8wcV",
        "outputId": "b252bb66-493e-4d50-fdf4-1cd014e89901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://packages.cloud.google.com/apt gcsfuse-jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 2s (139 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://packages.cloud.google.com/apt/dists/gcsfuse-jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 178 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update --fix-missing\n",
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2969,
          "status": "ok",
          "timestamp": 1744317540999,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "br-vd9DwA9sk",
        "outputId": "a44fc8ae-4d65-454d-a0b9-d16e424b247e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table link already exists.\n",
            "Table current_data_2 already exists.\n",
            "Table pfz already exists.\n",
            "Table wind_data already exists.\n",
            "Table current_data already exists.\n",
            "Table biogeochemical_data already exists.\n"
          ]
        }
      ],
      "source": [
        "# prompt: add code to create all the tables if they do not exist with proper column names and types as existing.\n",
        "\n",
        "import pandas_gbq\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = \"ocean-data-e68c2\"\n",
        "dataset_id = \"ocean_data\"\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "\n",
        "tables_schema = {\n",
        "    \"link\": [\n",
        "        {\"name\": \"id\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"english\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"tamil\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"image\", \"type\": \"STRING\"},\n",
        "    ],\n",
        "    \"current_data_2\": [\n",
        "        {\"name\": \"depth\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"latitude\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"longitude\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"time\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"thetao\", \"type\": \"FLOAT64\"},\n",
        "    ],\n",
        "    \"pfz\": [\n",
        "        {\"name\": \"WKT\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"name\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"description\", \"type\": \"STRING\"},\n",
        "    ],\n",
        "    \"wind_data\": [\n",
        "        {\"name\": \"time\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"latitude\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"longitude\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"eastward_wind\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"northward_wind\", \"type\": \"FLOAT64\"},\n",
        "    ],\n",
        "    \"current_data\": [\n",
        "        {\"name\": \"depth\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"latitude\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"longitude\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"time\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"uo\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"vo\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"fetch_date\", \"type\": \"DATE\"},\n",
        "    ],\n",
        "    \"biogeochemical_data\": [\n",
        "        {\"name\": \"depth\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"latitude\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"longitude\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"time\", \"type\": \"STRING\"},\n",
        "        {\"name\": \"nppv\", \"type\": \"FLOAT64\"},\n",
        "        {\"name\": \"o2\", \"type\": \"FLOAT64\"},\n",
        "    ],\n",
        "}\n",
        "\n",
        "\n",
        "for table_name, schema in tables_schema.items():\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "    try:\n",
        "        client.get_table(table_ref)\n",
        "        print(f\"Table {table_name} already exists.\")\n",
        "    except:\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        client.create_table(table)\n",
        "        print(f\"Table {table_name} created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 15434,
          "status": "ok",
          "timestamp": 1744317565303,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "ZF55yoKH0olQ",
        "outputId": "8fae4882-ba0c-4f6c-969f-2ad59a848e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: |\u001b[32m          \u001b[0m|\n",
            "Table current_data truncated successfully.\n",
            "Downloading: |\u001b[32m          \u001b[0m|\n",
            "Table biogeochemical_data truncated successfully.\n",
            "Downloading: |\u001b[32m          \u001b[0m|\n",
            "Table wind_data truncated successfully.\n",
            "Downloading: |\u001b[32m          \u001b[0m|\n",
            "Table current_data_2 truncated successfully.\n",
            "Error truncating table current_data_3: Reason: 404 POST https://bigquery.googleapis.com/bigquery/v2/projects/ocean-data-e68c2/queries?prettyPrint=false: Not found: Table ocean-data-e68c2:ocean_data.current_data_3 was not found in location US\n",
            "Downloading: |\u001b[32m          \u001b[0m|\n",
            "Table pfz truncated successfully.\n",
            "Downloading: |\u001b[32m          \u001b[0m|\n",
            "Table link truncated successfully.\n"
          ]
        }
      ],
      "source": [
        "# prompt: Truncate all tables in the ocean_data\n",
        "from google.cloud import bigquery\n",
        "import pandas_gbq\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client()\n",
        "dataset_id = \"ocean-data-e68c2.ocean_data\"\n",
        "\n",
        "# List of tables to truncate\n",
        "tables = [\n",
        "    \"current_data\",\n",
        "    \"biogeochemical_data\",\n",
        "    \"wind_data\",\n",
        "    \"current_data_2\",\n",
        "    \"current_data_3\",\n",
        "    \"pfz\",\n",
        "    \"link\"\n",
        "]\n",
        "\n",
        "for table in tables:\n",
        "    try:\n",
        "        # Construct the SQL query for truncation\n",
        "        sql = f\"TRUNCATE TABLE `{dataset_id}.{table}`\"\n",
        "\n",
        "        # Execute the query using pandas_gbq\n",
        "        pandas_gbq.read_gbq(sql, project_id=client.project, dialect=\"standard\")\n",
        "        print(f\"Table {table} truncated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error truncating table {table}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 95049,
          "status": "ok",
          "timestamp": 1744317686755,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "Y62RrXR7_XZR",
        "outputId": "db648fab-fd19-4e25-80f7-015f6e82884a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching current_data:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 2025-04-10T20:39:56Z - Selected dataset version: \"202406\"\n",
            "INFO:copernicusmarine:Selected dataset version: \"202406\"\n",
            "INFO - 2025-04-10T20:39:56Z - Selected dataset part: \"default\"\n",
            "INFO:copernicusmarine:Selected dataset part: \"default\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data fetched successfully:\n",
            "          depth   latitude  longitude       time        uo        vo\n",
            "0      0.494025   5.583336  71.750000 2025-04-11  0.085712 -0.206032\n",
            "1      0.494025   5.583336  71.833344 2025-04-11  0.052188 -0.292032\n",
            "2      0.494025   5.583336  71.916672 2025-04-11  0.016429 -0.368978\n",
            "3      0.494025   5.583336  72.000000 2025-04-11 -0.012949 -0.423768\n",
            "4      0.494025   5.583336  72.083344 2025-04-11 -0.038140 -0.471999\n",
            "...         ...        ...        ...        ...       ...       ...\n",
            "20281  0.494025  17.000000  83.583344 2025-04-11  0.051357  0.041299\n",
            "20282  0.494025  17.000000  83.666687 2025-04-11 -0.009885 -0.012147\n",
            "20283  0.494025  17.000000  83.750000 2025-04-11 -0.060266 -0.061246\n",
            "20284  0.494025  17.000000  83.833344 2025-04-11 -0.097096 -0.084607\n",
            "20285  0.494025  17.000000  83.916687 2025-04-11 -0.129645 -0.099479\n",
            "\n",
            "[13326 rows x 6 columns]\n",
            "Pushed data to ocean-data-e68c2.ocean_data.current_data\n",
            "\n",
            "Fetching biogeochemical_data:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 2025-04-10T20:40:19Z - Selected dataset version: \"202311\"\n",
            "INFO:copernicusmarine:Selected dataset version: \"202311\"\n",
            "INFO - 2025-04-10T20:40:19Z - Selected dataset part: \"default\"\n",
            "INFO:copernicusmarine:Selected dataset part: \"default\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data fetched successfully:\n",
            "         depth  latitude  longitude       time      nppv          o2\n",
            "0     0.494025      5.75      71.75 2025-04-11  5.126607  201.555084\n",
            "1     0.494025      5.75      72.00 2025-04-11  4.168730  200.748550\n",
            "2     0.494025      5.75      72.25 2025-04-11  4.513958  200.276367\n",
            "3     0.494025      5.75      72.50 2025-04-11  5.518563  200.506500\n",
            "4     0.494025      5.75      72.75 2025-04-11  5.744860  200.818024\n",
            "...        ...       ...        ...        ...       ...         ...\n",
            "2249  0.494025     17.00      82.75 2025-04-11  9.775278  204.352951\n",
            "2250  0.494025     17.00      83.00 2025-04-11  8.713862  204.348145\n",
            "2251  0.494025     17.00      83.25 2025-04-11  8.028949  204.287323\n",
            "2252  0.494025     17.00      83.50 2025-04-11  7.671774  203.875412\n",
            "2253  0.494025     17.00      83.75 2025-04-11  7.927057  204.096436\n",
            "\n",
            "[1457 rows x 6 columns]\n",
            "Pushed data to ocean-data-e68c2.ocean_data.biogeochemical_data\n",
            "\n",
            "Fetching wind_data:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 2025-04-10T20:40:42Z - Selected dataset version: \"202207\"\n",
            "INFO:copernicusmarine:Selected dataset version: \"202207\"\n",
            "INFO - 2025-04-10T20:40:42Z - Selected dataset part: \"default\"\n",
            "INFO:copernicusmarine:Selected dataset part: \"default\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data fetched successfully:\n",
            "                    time  latitude  longitude  eastward_wind  northward_wind\n",
            "0    2025-04-08 21:00:00    5.5625    71.6875           5.07            0.73\n",
            "1    2025-04-08 21:00:00    5.5625    71.8125           5.13            0.65\n",
            "2    2025-04-08 21:00:00    5.5625    71.9375           5.10            0.71\n",
            "3    2025-04-08 21:00:00    5.5625    72.0625           5.16            0.75\n",
            "4    2025-04-08 21:00:00    5.5625    72.1875           5.22            0.72\n",
            "...                  ...       ...        ...            ...             ...\n",
            "9202 2025-04-08 21:00:00   17.0625    83.4375          -5.19           -1.51\n",
            "9203 2025-04-08 21:00:00   17.0625    83.5625          -5.33           -1.21\n",
            "9204 2025-04-08 21:00:00   17.0625    83.6875          -5.46           -0.95\n",
            "9205 2025-04-08 21:00:00   17.0625    83.8125          -5.66           -0.44\n",
            "9206 2025-04-08 21:00:00   17.0625    83.9375          -5.87           -0.10\n",
            "\n",
            "[9207 rows x 5 columns]\n",
            "Pushed data to ocean-data-e68c2.ocean_data.wind_data\n",
            "\n",
            "Fetching current_data_2:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 2025-04-10T20:41:07Z - Selected dataset version: \"202406\"\n",
            "INFO:copernicusmarine:Selected dataset version: \"202406\"\n",
            "INFO - 2025-04-10T20:41:07Z - Selected dataset part: \"default\"\n",
            "INFO:copernicusmarine:Selected dataset part: \"default\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data fetched successfully:\n",
            "          depth   latitude  longitude       time     thetao\n",
            "0      0.494025   5.583336  71.750000 2025-04-11  30.080336\n",
            "1      0.494025   5.583336  71.833344 2025-04-11  30.014435\n",
            "2      0.494025   5.583336  71.916672 2025-04-11  30.051800\n",
            "3      0.494025   5.583336  72.000000 2025-04-11  30.231216\n",
            "4      0.494025   5.583336  72.083344 2025-04-11  30.378590\n",
            "...         ...        ...        ...        ...        ...\n",
            "20281  0.494025  17.000000  83.583344 2025-04-11  30.065073\n",
            "20282  0.494025  17.000000  83.666687 2025-04-11  30.023327\n",
            "20283  0.494025  17.000000  83.750000 2025-04-11  29.950909\n",
            "20284  0.494025  17.000000  83.833344 2025-04-11  29.894344\n",
            "20285  0.494025  17.000000  83.916687 2025-04-11  29.865007\n",
            "\n",
            "[13326 rows x 5 columns]\n",
            "Pushed data to ocean-data-e68c2.ocean_data.current_data_2\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "import copernicusmarine\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import datetime,timedelta\n",
        "a=datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client()\n",
        "dataset_id = \"ocean-data-e68c2.ocean_data\"\n",
        "a_wind = (datetime.now() - timedelta(days=2)).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "# Function to fetch and display data\n",
        "def fetch_and_display_data(query_params):\n",
        "    try:\n",
        "        df = copernicusmarine.read_dataframe(**query_params).reset_index().dropna()\n",
        "        if not df.empty:\n",
        "            print(\"Data fetched successfully:\")\n",
        "            print(df)  # Display the fetched DataFrame\n",
        "            return df  # Return the dataframe if needed for further processing.\n",
        "        else:\n",
        "            print(\"No data fetched for the specified parameters.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to push data to BigQuery\n",
        "def push_data_to_bigquery(df, dataset_id, table_name):\n",
        "    if df is not None:\n",
        "        try:\n",
        "            if \"time\" in df.columns:\n",
        "                df[\"time\"] = df[\"time\"].astype(str)\n",
        "            client.load_table_from_dataframe(df, f\"{dataset_id}.{table_name}\").result()\n",
        "            print(f\"Pushed data to {dataset_id}.{table_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error pushing data to BigQuery: {e}\")\n",
        "\n",
        "# Query parameters for each dataset\n",
        "current_data_params = {\n",
        "    \"dataset_id\": \"cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m\",\n",
        "    \"minimum_longitude\": 71.67651854792956,\n",
        "    \"maximum_longitude\": 83.99011977588334,\n",
        "    \"minimum_latitude\": 5.554121417215164,\n",
        "    \"maximum_latitude\": 17.074981057486614,\n",
        "    \"variables\": [\"uo\", \"vo\"],\n",
        "    \"start_datetime\": a,\n",
        "    \"end_datetime\": a,\n",
        "    \"minimum_depth\": 0.49402499198913574,\n",
        "    \"maximum_depth\": 0.49402499198913574,\n",
        "    \"username\": \"asas\",\n",
        "    \"password\": \"dssd\"\n",
        "}\n",
        "\n",
        "biogeochemical_data_params = {\n",
        "    \"dataset_id\": \"cmems_mod_glo_bgc-bio_anfc_0.25deg_P1D-m\",\n",
        "    \"variables\": [\"nppv\", \"o2\"],\n",
        "    \"minimum_longitude\": 71.67651854792956,\n",
        "    \"maximum_longitude\": 83.99011977588334,\n",
        "    \"minimum_latitude\": 5.554121417215164,\n",
        "    \"maximum_latitude\": 17.074981057486614,\n",
        "    \"start_datetime\":a,\n",
        "    \"end_datetime\": a,\n",
        "    \"minimum_depth\": 0.4940253794193268,\n",
        "    \"maximum_depth\": 0.4940253794193268,\n",
        "    \"username\": \"asas\",\n",
        "    \"password\": \"dssd\"\n",
        "}\n",
        "\n",
        "wind_data_params = {\n",
        "    \"dataset_id\": \"cmems_obs-wind_glo_phy_nrt_l4_0.125deg_PT1H\",\n",
        "    \"variables\": [\"eastward_wind\", \"northward_wind\"],\n",
        "    \"minimum_longitude\": 71.67651854792956,\n",
        "    \"maximum_longitude\": 83.99011977588334,\n",
        "    \"minimum_latitude\": 5.554121417215164,\n",
        "    \"maximum_latitude\": 17.074981057486614,\n",
        "    \"start_datetime\":a_wind,\n",
        "    \"end_datetime\": a_wind,\n",
        "    \"username\": \"asas\",\n",
        "    \"password\": \"dssd\"\n",
        "}\n",
        "\n",
        "current_data_2_params = {\n",
        "    \"dataset_id\": \"cmems_mod_glo_phy-thetao_anfc_0.083deg_P1D-m\",\n",
        "    \"variables\": [\"thetao\"],\n",
        "    \"minimum_longitude\": 71.67651854792956,\n",
        "    \"maximum_longitude\": 83.99011977588334,\n",
        "    \"minimum_latitude\": 5.554121417215164,\n",
        "    \"maximum_latitude\": 17.074981057486614,\n",
        "    \"start_datetime\": a,\n",
        "    \"end_datetime\": a,\n",
        "    \"minimum_depth\": 0.49402499198913574,\n",
        "    \"maximum_depth\": 0.49402499198913574,\n",
        "    \"username\": \"asas\",\n",
        "    \"password\": \"dssd\"\n",
        "}\n",
        "\n",
        "# Run each query separately:\n",
        "print(\"Fetching current_data:\")\n",
        "current_data_df = fetch_and_display_data(current_data_params)\n",
        "push_data_to_bigquery(current_data_df, dataset_id, \"current_data\")\n",
        "\n",
        "print(\"\\nFetching biogeochemical_data:\")\n",
        "biogeochemical_data_df = fetch_and_display_data(biogeochemical_data_params)\n",
        "push_data_to_bigquery(biogeochemical_data_df, dataset_id, \"biogeochemical_data\")\n",
        "\n",
        "print(\"\\nFetching wind_data:\")\n",
        "wind_data_df = fetch_and_display_data(wind_data_params)\n",
        "push_data_to_bigquery(wind_data_df, dataset_id, \"wind_data\")\n",
        "\n",
        "print(\"\\nFetching current_data_2:\")\n",
        "current_data_2_df = fetch_and_display_data(current_data_2_params)\n",
        "push_data_to_bigquery(current_data_2_df, dataset_id, \"current_data_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 15239,
          "status": "ok",
          "timestamp": 1744317701991,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "rWpdWyk9qNcA",
        "outputId": "51902d90-f9a2-481e-a7d7-fbaa8c5280e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 2025-04-10T20:41:29Z - Selected dataset version: \"202311\"\n",
            "INFO:copernicusmarine:Selected dataset version: \"202311\"\n",
            "INFO - 2025-04-10T20:41:29Z - Selected dataset part: \"default\"\n",
            "INFO:copernicusmarine:Selected dataset part: \"default\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fetching wind data: Some of your subset selection [2025-04-10 20:41:26+00:00, 2025-04-10 20:41:26+00:00] for the time dimension exceed the dataset coordinates [2020-11-11 00:00:00+00:00, 2025-04-09 00:00:00+00:00]\n"
          ]
        }
      ],
      "source": [
        "#wind data\n",
        "from google.cloud import bigquery\n",
        "import copernicusmarine\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current timestamp in the required format\n",
        "current_time = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client()\n",
        "dataset_id = \"ocean-data-e68c2.ocean_data\"\n",
        "\n",
        "# Function to fetch and display wind data\n",
        "def fetch_wind_data():\n",
        "    wind_data_params = {\n",
        "        \"dataset_id\": \"cmems_obs-wind_glo_phy_nrt_l3-hy2b-hscat-asc-0.25deg_P1D-i\",\n",
        "        \"variables\": [\"eastward_wind\", \"northward_wind\"],\n",
        "        \"minimum_longitude\": 71.67651854792956,\n",
        "        \"maximum_longitude\": 83.99011977588334,\n",
        "        \"minimum_latitude\": 5.554121417215164,\n",
        "        \"maximum_latitude\": 17.074981057486614,\n",
        "        \"start_datetime\": current_time,\n",
        "        \"end_datetime\": current_time,\n",
        "        \"username\": \"asas\",\n",
        "        \"password\": \"dssd\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        df = copernicusmarine.read_dataframe(**wind_data_params).reset_index().dropna()\n",
        "        if not df.empty:\n",
        "            print(\"Wind Data Fetched Successfully:\")\n",
        "            print(df)\n",
        "            return df\n",
        "        else:\n",
        "            print(\"No wind data found for the specified parameters.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching wind data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Fetch wind data\n",
        "wind_data_df = fetch_wind_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2563,
          "status": "ok",
          "timestamp": 1744317704551,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "rSKf54H3F_SQ",
        "outputId": "e0af2e45-c933-494e-a8a9-4cd148773bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame pushed to ocean-data-e68c2.ocean_data.pfz successfully.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from geopy.distance import geodesic\n",
        "from itertools import combinations\n",
        "import pandas_gbq\n",
        "from google.cloud import bigquery\n",
        "# Step 1: Define base URL and target URL\n",
        "base_url = \"https://incois.gov.in/MarineFisheries/\"\n",
        "target_url = \"https://incois.gov.in/MarineFisheries/TextDataHome?mfid=1&request_locale=en#\"\n",
        "\n",
        "# Step 2: Set headers to mimic a browser request\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
        "}\n",
        "\n",
        "# Step 3: Fetch the main page\n",
        "session = requests.Session()\n",
        "response = session.get(target_url, headers=headers)\n",
        "\n",
        "# Step 4: Parse the HTML content using BeautifulSoup\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Step 5: Find the link to \"SOUTH TAMILNADU\"\n",
        "    south_tn_link = None\n",
        "    for link in soup.find_all(\"a\", href=True):\n",
        "        if \"NORTH TAMILNADU\" in link.text:\n",
        "            south_tn_link = base_url + link[\"href\"]\n",
        "            break\n",
        "\n",
        "    # Step 6: If the link is found, fetch the data\n",
        "    if south_tn_link:\n",
        "        south_tn_response = session.get(south_tn_link, headers=headers)\n",
        "\n",
        "        if south_tn_response.status_code == 200:\n",
        "            pass\n",
        "            #print(\"Data from SOUTH TAMILNADU page:\\n\")\n",
        "            #print(south_tn_response.text)  # Prints extracted page content\n",
        "        else:\n",
        "            print(f\"Failed to load SOUTH TAMILNADU page. Status Code: {south_tn_response.status_code}\")\n",
        "    else:\n",
        "        print(\"Could not find the SOUTH TAMILNADU link on the page.\")\n",
        "else:\n",
        "    print(f\"Failed to fetch the main page. Status Code: {response.status_code}\")\n",
        "\n",
        "html_content = south_tn_response.text\n",
        "\n",
        "# Parse the HTML with BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "# Find all table rows\n",
        "rows = soup.find_all(\"tr\")\n",
        "\n",
        "# Initialize lists for storing latitude and longitude\n",
        "latitudes = []\n",
        "longitudes = []\n",
        "\n",
        "# Iterate over rows and extract latitude & longitude\n",
        "for row in rows:\n",
        "    cells = row.find_all(\"td\")\n",
        "    if len(cells) >= 7:  # Ensure there are enough columns\n",
        "        latitudes.append(cells[-2].text.strip())  # Second last column for latitude\n",
        "        longitudes.append(cells[-1].text.strip())  # Last column for longitude\n",
        "\n",
        "# Convert to a Pandas DataFrame\n",
        "df = pd.DataFrame({\"Latitude\": latitudes, \"Longitude\": longitudes})\n",
        "\n",
        "def dms_to_dd(dms_str):\n",
        "    parts = dms_str.split()\n",
        "    degrees = float(parts[0])\n",
        "    minutes = float(parts[1])\n",
        "    seconds = float(parts[2])\n",
        "    direction = parts[3]  # 'N', 'S', 'E', 'W'\n",
        "\n",
        "    decimal = degrees + (minutes / 60) + (seconds / 3600)\n",
        "\n",
        "    # Convert to negative for South and West directions\n",
        "    if direction in ['S', 'W']:\n",
        "        decimal = -decimal\n",
        "\n",
        "    return decimal\n",
        "\n",
        "# Convert Latitude and Longitude to Decimal Degrees\n",
        "df['Latitude_DD'] = df['Latitude'].apply(dms_to_dd)\n",
        "df['Longitude_DD'] = df['Longitude'].apply(dms_to_dd)\n",
        "\n",
        "\n",
        "\n",
        "lat_col = \"Latitude_DD\"  # Replace with actual column name if different\n",
        "lon_col = \"Longitude_DD\"  # Replace with actual column name if different\n",
        "\n",
        "# Create a list to store WKT lines\n",
        "wkt_lines = []\n",
        "line_counter = 1  # Naming the lines sequentially\n",
        "\n",
        "# Iterate through all unique pairs of coordinates\n",
        "for (index1, row1), (index2, row2) in combinations(df.iterrows(), 2):\n",
        "    coord1 = (row1[lat_col], row1[lon_col])\n",
        "    coord2 = (row2[lat_col], row2[lon_col])\n",
        "\n",
        "    # Calculate distance between two points\n",
        "    distance = geodesic(coord1, coord2).kilometers\n",
        "\n",
        "    # Connect points if they are within 3.5 km\n",
        "    if distance <= 5:\n",
        "        wkt_format = f\"LINESTRING ({row1[lon_col]} {row1[lat_col]}, {row2[lon_col]} {row2[lat_col]})\"\n",
        "        wkt_lines.append([wkt_format, f\"Line {line_counter}\", \"\"])\n",
        "        line_counter += 1\n",
        "\n",
        "# Create a DataFrame with WKT format\n",
        "wkt_df1 = pd.DataFrame(wkt_lines, columns=[\"WKT\", \"name\", \"description\"])\n",
        "\n",
        "\n",
        "\n",
        "base_url = \"https://incois.gov.in/MarineFisheries/\"\n",
        "target_url = \"https://incois.gov.in/MarineFisheries/TextDataHome?mfid=1&request_locale=en#\"\n",
        "\n",
        "# Step 2: Set headers to mimic a browser request\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
        "}\n",
        "\n",
        "# Step 3: Fetch the main page\n",
        "session = requests.Session()\n",
        "response = session.get(target_url, headers=headers)\n",
        "\n",
        "# Step 4: Parse the HTML content using BeautifulSoup\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Step 5: Find the link to \"SOUTH TAMILNADU\"\n",
        "    south_tn_link = None\n",
        "    for link in soup.find_all(\"a\", href=True):\n",
        "        if \"SOUTH TAMILNADU\" in link.text:\n",
        "            south_tn_link = base_url + link[\"href\"]\n",
        "            break\n",
        "\n",
        "    # Step 6: If the link is found, fetch the data\n",
        "    if south_tn_link:\n",
        "        south_tn_response = session.get(south_tn_link, headers=headers)\n",
        "\n",
        "        if south_tn_response.status_code == 200:\n",
        "            pass\n",
        "            #print(\"Data from SOUTH TAMILNADU page:\\n\")\n",
        "            #print(south_tn_response.text)  # Prints extracted page content\n",
        "        else:\n",
        "            print(f\"Failed to load SOUTH TAMILNADU page. Status Code: {south_tn_response.status_code}\")\n",
        "    else:\n",
        "        print(\"Could not find the SOUTH TAMILNADU link on the page.\")\n",
        "else:\n",
        "    print(f\"Failed to fetch the main page. Status Code: {response.status_code}\")\n",
        "\n",
        "html_content = south_tn_response.text\n",
        "\n",
        "# Parse the HTML with BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "# Find all table rows\n",
        "rows = soup.find_all(\"tr\")\n",
        "\n",
        "# Initialize lists for storing latitude and longitude\n",
        "latitudes = []\n",
        "longitudes = []\n",
        "\n",
        "# Iterate over rows and extract latitude & longitude\n",
        "for row in rows:\n",
        "    cells = row.find_all(\"td\")\n",
        "    if len(cells) >= 7:  # Ensure there are enough columns\n",
        "        latitudes.append(cells[-2].text.strip())  # Second last column for latitude\n",
        "        longitudes.append(cells[-1].text.strip())  # Last column for longitude\n",
        "\n",
        "# Convert to a Pandas DataFrame\n",
        "df = pd.DataFrame({\"Latitude\": latitudes, \"Longitude\": longitudes})\n",
        "\n",
        "def dms_to_dd(dms_str):\n",
        "    parts = dms_str.split()\n",
        "    degrees = float(parts[0])\n",
        "    minutes = float(parts[1])\n",
        "    seconds = float(parts[2])\n",
        "    direction = parts[3]  # 'N', 'S', 'E', 'W'\n",
        "\n",
        "    decimal = degrees + (minutes / 60) + (seconds / 3600)\n",
        "\n",
        "    # Convert to negative for South and West directions\n",
        "    if direction in ['S', 'W']:\n",
        "        decimal = -decimal\n",
        "\n",
        "    return decimal\n",
        "\n",
        "# Convert Latitude and Longitude to Decimal Degrees\n",
        "df['Latitude_DD'] = df['Latitude'].apply(dms_to_dd)\n",
        "df['Longitude_DD'] = df['Longitude'].apply(dms_to_dd)\n",
        "\n",
        "\n",
        "\n",
        "lat_col = \"Latitude_DD\"  # Replace with actual column name if different\n",
        "lon_col = \"Longitude_DD\"  # Replace with actual column name if different\n",
        "\n",
        "# Create a list to store WKT lines\n",
        "wkt_lines = []\n",
        "line_counter = 1  # Naming the lines sequentially\n",
        "\n",
        "# Iterate through all unique pairs of coordinates\n",
        "for (index1, row1), (index2, row2) in combinations(df.iterrows(), 2):\n",
        "    coord1 = (row1[lat_col], row1[lon_col])\n",
        "    coord2 = (row2[lat_col], row2[lon_col])\n",
        "\n",
        "    # Calculate distance between two points\n",
        "    distance = geodesic(coord1, coord2).kilometers\n",
        "\n",
        "    # Connect points if they are within 3.5 km\n",
        "    if distance <= 5:\n",
        "        wkt_format = f\"LINESTRING ({row1[lon_col]} {row1[lat_col]}, {row2[lon_col]} {row2[lat_col]})\"\n",
        "        wkt_lines.append([wkt_format, f\"Line {line_counter}\", \"\"])\n",
        "        line_counter += 1\n",
        "\n",
        "# Create a DataFrame with WKT format\n",
        "wkt_df2 = pd.DataFrame(wkt_lines, columns=[\"WKT\", \"name\", \"description\"])\n",
        "merged_df = pd.concat([wkt_df1, wkt_df2], ignore_index=True)\n",
        "merged_df['name'] = ['Line ' + str(i+1) for i in range(len(merged_df))]\n",
        "output_file = \"FINAL_LINES.csv\"\n",
        "client = bigquery.Client()\n",
        "dataset_id = \"ocean-data-e68c2.ocean_data\"\n",
        "table_name = \"pfz\"\n",
        "\n",
        "try:\n",
        "    pandas_gbq.to_gbq(merged_df, f\"{dataset_id}.{table_name}\", project_id=client.project, if_exists='replace')\n",
        "    print(f\"DataFrame pushed to {dataset_id}.{table_name} successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error pushing DataFrame to BigQuery: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "executionInfo": {
          "elapsed": 231587,
          "status": "ok",
          "timestamp": 1744317936116,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -330
        },
        "id": "p-0cqUlIBNxs",
        "outputId": "2bc6e1cf-4f8a-4f45-8d4a-9ff190828019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Downloading PDF...\n",
            " Converting PDF pages to images...\n",
            " Processing page 1...\n",
            " Processing page 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.0-flash-thinking-exp-01-21:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3146.20ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing page 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.0-flash-thinking-exp-01-21:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2892.40ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing page 4...\n",
            " Processing page 5...\n",
            " Uploading to BigQuery...\n",
            " All data inserted successfully into BigQuery!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "from pdf2image import convert_from_bytes\n",
        "from PIL import Image\n",
        "import base64\n",
        "from google.cloud import bigquery\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "GEMINI_API_KEY = 'YOUR_GEMINI_API_KEY'  # Replace with your actual API key\n",
        "# PDF URL\n",
        "PDF_URL = \"https://mausam.imd.gov.in/chennai/mcdata/fishermen.pdf\"\n",
        "BIGQUERY_TABLE = \"ocean-data-e68c2.ocean_data.link\"\n",
        "\n",
        "# ---------- AUTH ----------\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Use gemini-pro-vision for image input\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash-thinking-exp-01-21\")\n",
        "\n",
        "# BigQuery client\n",
        "bq_client = bigquery.Client()\n",
        "\n",
        "# ---------- DOWNLOAD PDF ----------\n",
        "print(\" Downloading PDF...\")\n",
        "response = requests.get(PDF_URL)\n",
        "pdf_data = BytesIO(response.content)\n",
        "\n",
        "# ---------- CONVERT TO IMAGES ----------\n",
        "print(\" Converting PDF pages to images...\")\n",
        "images = convert_from_bytes(pdf_data.read())\n",
        "\n",
        "# ---------- PROCESS EACH PAGE ----------\n",
        "rows_to_insert = []\n",
        "\n",
        "for i, img in enumerate(images, start=1):\n",
        "    print(f\" Processing page {i}...\")\n",
        "\n",
        "    # Convert image to bytes\n",
        "    buffered = BytesIO()\n",
        "    img.save(buffered, format=\"PNG\")\n",
        "    image_bytes = buffered.getvalue()\n",
        "\n",
        "    # Encode image to base64 for BigQuery storage\n",
        "    img_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "\n",
        "    # Gemini Summary - English\n",
        "    try:\n",
        "        english_summary = model.generate_content([\n",
        "            \"Summarize this weather bulletin page in English clearly:\",\n",
        "            {\"mime_type\": \"image/png\", \"data\": image_bytes}\n",
        "        ]).text.strip()\n",
        "    except Exception as e:\n",
        "        english_summary = f\"Error generating English summary: {str(e)}\"\n",
        "\n",
        "    # Gemini Summary - Tamil\n",
        "    try:\n",
        "        tamil_summary = model.generate_content([\n",
        "            \"    .     :\",\n",
        "            {\"mime_type\": \"image/png\", \"data\": image_bytes}\n",
        "        ]).text.strip()\n",
        "    except Exception as e:\n",
        "        tamil_summary = f\"   : {str(e)}\"\n",
        "\n",
        "    # Prepare row for BigQuery\n",
        "    row = {\n",
        "        \"id\": i,\n",
        "        \"english\": english_summary,\n",
        "        \"tamil\": tamil_summary,\n",
        "        \"image\": img_base64\n",
        "    }\n",
        "    rows_to_insert.append(row)\n",
        "\n",
        "# ---------- PUSH TO BIGQUERY ----------\n",
        "print(\" Uploading to BigQuery...\")\n",
        "errors = bq_client.insert_rows_json(BIGQUERY_TABLE, rows_to_insert)\n",
        "\n",
        "if not errors:\n",
        "    print(\" All data inserted successfully into BigQuery!\")\n",
        "else:\n",
        "    print(\" Errors occurred while inserting into BigQuery:\")\n",
        "    print(errors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XUp09Ep86-3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "name": "Copernicus_fetch",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
